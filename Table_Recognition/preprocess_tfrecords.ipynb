{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tfrecord\n",
    "import cv2\n",
    "from time import process_time\n",
    "import tensorflow as tf\n",
    "from tfrecord.torch.dataset import TFRecordDataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visibility_matrix(torch_df,num_words):\n",
    "    '''indentify neighbours to the right and down and generate visibility matrix / neighbourhood graph.\n",
    "        for each node, we indentify it's closest neighbour to the right and the closest neighbour below.\n",
    "    input: numpy array of shape (words, [x1, x2, y1, y2])\n",
    "    output: visibility matrix of shape (words, words)'''\n",
    "    \n",
    "    #remove last column (word_length)\n",
    "    npdf = torch_df.numpy()\n",
    "    \n",
    "    #Only create matrix of size matching number of words\n",
    "    matrix = np.zeros((num_words, num_words))\n",
    "\n",
    "    for i,row1 in enumerate(npdf):\n",
    "        if i == num_words:\n",
    "            break\n",
    "\n",
    "        #xmin = 0\n",
    "        #ymin = 1\n",
    "        #xmax = 2\n",
    "        #ymax = 3 \n",
    "\n",
    "        min_down = 10**6\n",
    "        min_right = 10**6\n",
    "        min_down_idx = None\n",
    "        min_right_idx = None\n",
    "\n",
    "        for j,row2 in enumerate(npdf):\n",
    "            if j == num_words:\n",
    "                break\n",
    "            if i != j:\n",
    "                #Right neighbour\n",
    "                if row1[1] <= row2[1] <= row1[3] or row1[1] <= row2[3] <= row1[3] or row2[1] <= row1[1] <= row2[3] or row2[1] <= row1[3] <= row2[3]:\n",
    "                    if  0 <= row2[0]-row1[2] <= min_right:\n",
    "                        min_right_idx, min_right = j, row2[0]-row1[2]\n",
    "\n",
    "                #Down neighbour\n",
    "                if row1[0] <= row2[0] <= row1[2] or row1[0] <= row2[2] <= row1[2] or row2[0] <= row1[0] <= row2[2] or row2[0] <= row1[2] <= row2[2]:\n",
    "                    if 0 <= row2[1]-row1[3] <= min_down:\n",
    "                        min_down_idx, min_down = j, row2[1]-row1[3]\n",
    "\n",
    "        if min_right_idx != None:\n",
    "            matrix[i,min_right_idx] = 1\n",
    "            matrix[min_right_idx, i] = 1    \n",
    "        if min_down_idx != None:\n",
    "            matrix[i,min_down_idx] = 1\n",
    "            matrix[min_down_idx, i] = 1\n",
    "            \n",
    "    source = []\n",
    "    target = []\n",
    "\n",
    "    for i, row in enumerate(matrix):\n",
    "        for j, edge in enumerate(row):\n",
    "            if edge == 1:\n",
    "                source.append(i)\n",
    "                target.append(j)\n",
    "\n",
    "    edge_index = torch.tensor([source, target], dtype=torch.long)\n",
    "\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecord_transforms(elem,\n",
    "                   max_height = 768,\n",
    "                   max_width = 1366,\n",
    "                   num_of_max_vertices = 250,\n",
    "                   max_length_of_word = 30,\n",
    "                   batch_size = 8):\n",
    "    \"\"\"\n",
    "    Function used to transform the data loaded by the TFRecord dataloader.\n",
    "    Parameters are defind in TIES datageneration, defines the size and complexity of the generated tables. DO NOT CHANGE  \n",
    "    \"\"\"\n",
    "    reshape = 0\n",
    "    xnumwords = 0\n",
    "    feat_reshap = 0\n",
    "    visimat = 0\n",
    "    adjmats = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #Everything is flattened in tfrecord, so needs to be reshaped. \n",
    "\n",
    "        #Images are in range [0,255], need to be in [0,1]\n",
    "        #If image max is over 1 , then normalize: \n",
    "        data_dict =  {}\n",
    "\n",
    "        \n",
    "        #Torch dimensions: B x C x H x W\n",
    "        #inputting grayscale, so only 1 dimension\n",
    "        t = process_time()\n",
    "        if torch.max(elem['image']) > 1:\n",
    "            data_dict['imgs'] = (elem['image']/255).reshape(batch_size,1,max_height,max_width)\n",
    "        else:\n",
    "            data_dict['imgs'] = elem['image'].reshape(batch_size,1,max_height,max_width)\n",
    "        reshape+=process_time()-t\n",
    "\n",
    "        #Extract number of words for each image:\n",
    "        t = process_time()\n",
    "        num_words = elem['global_features'][:,2]\n",
    "        data_dict['num_words'] = num_words\n",
    "        xnumwords += process_time()-t\n",
    "        \n",
    "        t = process_time()\n",
    "        v = elem['vertex_features'].reshape(batch_size,num_of_max_vertices,5).float()\n",
    "        feat_reshap += process_time()-t\n",
    "        #normalizaing words coordinates to be invariant to image size \n",
    "        v[:,:,0] = v[:,:,0]/max_width\n",
    "        v[:,:,1] = v[:,:,1]/max_height\n",
    "        v[:,:,2] = v[:,:,2]/max_width\n",
    "        v[:,:,3] = v[:,:,3]/max_height\n",
    "\n",
    "        #data_dict['vertex_features'] = v\n",
    "\n",
    "        vertex_feats = []\n",
    "        for idx,vf in enumerate(v):\n",
    "          \n",
    "            vertex_feats.append(vf.numpy())\n",
    "\n",
    "        data_dict['vertex_features'] = vertex_feats  \n",
    "                \n",
    "        #Calculate visibility matrix for each batch element\n",
    "        t = process_time()\n",
    "        edge_index = []\n",
    "        for idx,vex in enumerate(v):\n",
    "            edge_index.append(visibility_matrix(vex,num_words[idx]))\n",
    "        visimat += process_time()-t\n",
    "         \n",
    "        data_dict['edge_index'] = edge_index\n",
    "\n",
    "        \n",
    "        adj_cells = []\n",
    "        adj_cols = []\n",
    "        adj_rows = []\n",
    "        for idx,nw in enumerate(num_words):\n",
    "            adj_cells.append(elem['adjacency_matrix_cells'][idx].reshape(num_of_max_vertices,num_of_max_vertices).numpy())\n",
    "            adj_cols.append(elem['adjacency_matrix_cols'][idx].reshape(num_of_max_vertices,num_of_max_vertices).numpy())\n",
    "            adj_rows.append(elem['adjacency_matrix_rows'][idx].reshape(num_of_max_vertices,num_of_max_vertices).numpy())\n",
    "\n",
    "        data_dict['adjacency_matrix_cells'] = adj_cells\n",
    "        data_dict['adjacency_matrix_cols'] = adj_cols\n",
    "        data_dict['adjacency_matrix_rows'] = adj_rows\n",
    "        \n",
    "\n",
    "        \n",
    "        #print(f'#####TRANSFORMS: reshape: {reshape}, extract number of words: {xnumwords}, feat_reshape: {feat_reshap}, visibility matrix: {visimat}, adjacency matrix: {adjmats}')\n",
    "\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables for tfrecord loader\n",
    "batchsize = 8\n",
    "index_path = None\n",
    "tfrecord_description = {\"image\": \"float\", \n",
    "               \"global_features\": \"int\",\n",
    "               \"vertex_features\": \"int\",\n",
    "               \"adjacency_matrix_cells\":\"int\",\n",
    "               \"adjacency_matrix_cols\":\"int\",\n",
    "               \"adjacency_matrix_rows\":\"int\",\n",
    "               \"vertex_text\":'int'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load list of tfRecords from folder: \n",
    "folder_path = os.getcwd()+r'\\tfrecords'\n",
    "#folder_path = \"C:\\Users\\Jesper\\Desktop\\DataGeneration\\Data_Outputs\"\n",
    "\n",
    "#load filenames of folder: \n",
    "tfrecord_files = os.listdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outtfpath = os.getcwd()+r'\\processed_tfrecords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imgs: float\n",
    "#num_words: int\n",
    "#vf: float\n",
    "#edge index: int\n",
    "#adj int \n",
    "\n",
    "options = tf.compat.v1.io.TFRecordOptions(tf.compat.v1.io.TFRecordCompressionType.GZIP)\n",
    "\n",
    "for idx, record in enumerate(tfrecord_files):\n",
    "    tfrecord_path = os.path.join(folder_path,record)\n",
    "    dataset = TFRecordDataset(tfrecord_path, index_path, tfrecord_description)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batchsize)\n",
    "    for elem in loader:\n",
    "        dd = tfrecord_transforms(elem,batch_size=batchsize)\n",
    "        #\n",
    "        imgs = dd['imgs'].numpy()\n",
    "        \n",
    "        #\n",
    "        nw = dd['num_words'].numpy()\n",
    "        \n",
    "        #\n",
    "        vf = np.array(dd['vertex_features'])\n",
    "        \n",
    "        #\n",
    "        edge_num = []\n",
    "        for e in dd['edge_index']:\n",
    "            edge_num.append(e.shape[1])\n",
    "        \n",
    "        edge_num = np.array(edge_num,dtype=int)\n",
    "        \n",
    "        #\n",
    "        edge_indexes = []\n",
    "        maxed = np.max(edge_num)\n",
    "        for i,ed in enumerate(dd['edge_index']):\n",
    "            a = np.zeros(maxed)\n",
    "            b = np.zeros(maxed)\n",
    "\n",
    "            a[:edge_num[i]] = ed[0]\n",
    "            b[:edge_num[i]] = ed[1]\n",
    "\n",
    "            edge_indexes.append(np.array([a,b],dtype=int))\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        out = dict()\n",
    "        \n",
    "        out['imgs'] = tf.train.Feature(float_list=tf.train.FloatList(value=imgs.flatten()))\n",
    "        out['num_words'] = tf.train.Feature(int64_list=tf.train.Int64List(value=nw.flatten()))\n",
    "        out['vertex_features'] = tf.train.Feature(float_list=tf.train.FloatList(value=vf.flatten()))\n",
    "        out['num_edges'] =  tf.train.Feature(int64_list=tf.train.Int64List(value=edge_num.flatten()))\n",
    "        out['edge_indexes'] =  tf.train.Feature(int64_list=tf.train.Int64List(value=np.array(edge_indexes).flatten()))\n",
    "        \n",
    "        out['adjacency_matrix_cells'] = tf.train.Feature(int64_list=tf.train.Int64List(value=np.array(dd['adjacency_matrix_cells']).flatten()))\n",
    "        out['adjacency_matrix_cols'] = tf.train.Feature(int64_list=tf.train.Int64List(value=np.array(dd['adjacency_matrix_cols']).flatten()))\n",
    "        out['adjacency_matrix_rows'] = tf.train.Feature(int64_list=tf.train.Int64List(value=np.array(dd['adjacency_matrix_rows']).flatten()))\n",
    "        \n",
    "    with tf.io.TFRecordWriter(os.path.join(outtfpath,record+\".gz\"),options=options) as writer:\n",
    "        \n",
    "        all_features = tf.train.Features(feature=out)\n",
    "\n",
    "\n",
    "        seq_ex = tf.train.Example(features=all_features)\n",
    "        writer.write(seq_ex.SerializeToString()) \n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
