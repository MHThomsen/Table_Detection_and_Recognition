{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tfrecord\n",
    "import cv2\n",
    "from time import process_time\n",
    "import tensorflow as tf\n",
    "from tfrecord.torch.dataset import TFRecordDataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visibility_matrix(torch_df,num_words):\n",
    "    '''indentify neighbours to the right and down and generate visibility matrix / neighbourhood graph.\n",
    "        for each node, we indentify it's closest neighbour to the right and the closest neighbour below.\n",
    "    input: numpy array of shape (words, [x1, x2, y1, y2])\n",
    "    output: visibility matrix of shape (words, words)'''\n",
    "    \n",
    "    #remove last column (word_length)\n",
    "    npdf = torch_df.numpy()\n",
    "    \n",
    "    #Only create matrix of size matching number of words\n",
    "    matrix = np.zeros((num_words, num_words))\n",
    "\n",
    "    for i,row1 in enumerate(npdf):\n",
    "        if i == num_words:\n",
    "            break\n",
    "\n",
    "        #xmin = 0\n",
    "        #ymin = 1\n",
    "        #xmax = 2\n",
    "        #ymax = 3 \n",
    "\n",
    "        min_down = 10**6\n",
    "        min_right = 10**6\n",
    "        min_down_idx = None\n",
    "        min_right_idx = None\n",
    "\n",
    "        for j,row2 in enumerate(npdf):\n",
    "            if j == num_words:\n",
    "                break\n",
    "            if i != j:\n",
    "                #Right neighbour\n",
    "                if row1[1] <= row2[1] <= row1[3] or row1[1] <= row2[3] <= row1[3] or row2[1] <= row1[1] <= row2[3] or row2[1] <= row1[3] <= row2[3]:\n",
    "                    if  0 <= row2[0]-row1[2] <= min_right:\n",
    "                        min_right_idx, min_right = j, row2[0]-row1[2]\n",
    "\n",
    "                #Down neighbour\n",
    "                if row1[0] <= row2[0] <= row1[2] or row1[0] <= row2[2] <= row1[2] or row2[0] <= row1[0] <= row2[2] or row2[0] <= row1[2] <= row2[2]:\n",
    "                    if 0 <= row2[1]-row1[3] <= min_down:\n",
    "                        min_down_idx, min_down = j, row2[1]-row1[3]\n",
    "\n",
    "        if min_right_idx != None:\n",
    "            matrix[i,min_right_idx] = 1\n",
    "            matrix[min_right_idx, i] = 1    \n",
    "        if min_down_idx != None:\n",
    "            matrix[i,min_down_idx] = 1\n",
    "            matrix[min_down_idx, i] = 1\n",
    "            \n",
    "    source = []\n",
    "    target = []\n",
    "\n",
    "    for i, row in enumerate(matrix):\n",
    "        for j, edge in enumerate(row):\n",
    "            if edge == 1:\n",
    "                source.append(i)\n",
    "                target.append(j)\n",
    "\n",
    "    edge_index = torch.tensor([source, target], dtype=torch.long)\n",
    "\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecord_transforms(elem,\n",
    "                   max_height = 768,\n",
    "                   max_width = 1366,\n",
    "                   num_of_max_vertices = 250,\n",
    "                   max_length_of_word = 30,\n",
    "                   batch_size = 8):\n",
    "    \"\"\"\n",
    "    Function used to transform the data loaded by the TFRecord dataloader.\n",
    "    Parameters are defind in TIES datageneration, defines the size and complexity of the generated tables. DO NOT CHANGE  \n",
    "    \"\"\"\n",
    "    reshape = 0\n",
    "    xnumwords = 0\n",
    "    feat_reshap = 0\n",
    "    visimat = 0\n",
    "    adjmats = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #Everything is flattened in tfrecord, so needs to be reshaped. \n",
    "\n",
    "        #Images are in range [0,255], need to be in [0,1]\n",
    "        #If image max is over 1 , then normalize: \n",
    "        data_dict =  {}\n",
    "\n",
    "        \n",
    "        #Torch dimensions: B x C x H x W\n",
    "        #inputting grayscale, so only 1 dimension\n",
    "        t = process_time()\n",
    "        if torch.max(elem['image']) > 1:\n",
    "            data_dict['imgs'] = (elem['image']/255).reshape(batch_size,1,max_height,max_width)\n",
    "        else:\n",
    "            data_dict['imgs'] = elem['image'].reshape(batch_size,1,max_height,max_width)\n",
    "        reshape+=process_time()-t\n",
    "\n",
    "        #Extract number of words for each image:\n",
    "        t = process_time()\n",
    "        num_words = elem['global_features'][:,2]\n",
    "        data_dict['num_words'] = num_words\n",
    "        xnumwords += process_time()-t\n",
    "        \n",
    "        t = process_time()\n",
    "        v = elem['vertex_features'].reshape(batch_size,num_of_max_vertices,5).float()\n",
    "        feat_reshap += process_time()-t\n",
    "        #normalizaing words coordinates to be invariant to image size \n",
    "        v[:,:,0] = v[:,:,0]/max_width\n",
    "        v[:,:,1] = v[:,:,1]/max_height\n",
    "        v[:,:,2] = v[:,:,2]/max_width\n",
    "        v[:,:,3] = v[:,:,3]/max_height\n",
    "\n",
    "        #data_dict['vertex_features'] = v\n",
    "\n",
    "        vertex_feats = []\n",
    "        for idx,vf in enumerate(v):\n",
    "          \n",
    "            vertex_feats.append(vf.numpy())\n",
    "\n",
    "        data_dict['vertex_features'] = vertex_feats  \n",
    "                \n",
    "        #Calculate visibility matrix for each batch element\n",
    "        t = process_time()\n",
    "        edge_index = []\n",
    "        for idx,vex in enumerate(v):\n",
    "            edge_index.append(visibility_matrix(vex,num_words[idx]))\n",
    "        visimat += process_time()-t\n",
    "         \n",
    "        data_dict['edge_index'] = edge_index\n",
    "\n",
    "        \n",
    "        adj_cells = []\n",
    "        adj_cols = []\n",
    "        adj_rows = []\n",
    "        for idx,nw in enumerate(num_words):\n",
    "            adj_cells.append(elem['adjacency_matrix_cells'][idx].reshape(num_of_max_vertices,num_of_max_vertices).numpy())\n",
    "            adj_cols.append(elem['adjacency_matrix_cols'][idx].reshape(num_of_max_vertices,num_of_max_vertices).numpy())\n",
    "            adj_rows.append(elem['adjacency_matrix_rows'][idx].reshape(num_of_max_vertices,num_of_max_vertices).numpy())\n",
    "\n",
    "        data_dict['adjacency_matrix_cells'] = adj_cells\n",
    "        data_dict['adjacency_matrix_cols'] = adj_cols\n",
    "        data_dict['adjacency_matrix_rows'] = adj_rows\n",
    "        \n",
    "\n",
    "        \n",
    "        #print(f'#####TRANSFORMS: reshape: {reshape}, extract number of words: {xnumwords}, feat_reshape: {feat_reshap}, visibility matrix: {visimat}, adjacency matrix: {adjmats}')\n",
    "\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables for tfrecord loader\n",
    "batchsize = 8\n",
    "index_path = None\n",
    "tfrecord_description = {\"image\": \"float\", \n",
    "               \"global_features\": \"int\",\n",
    "               \"vertex_features\": \"int\",\n",
    "               \"adjacency_matrix_cells\":\"int\",\n",
    "               \"adjacency_matrix_cols\":\"int\",\n",
    "               \"adjacency_matrix_rows\":\"int\",\n",
    "               \"vertex_text\":'int'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load list of tfRecords from folder: \n",
    "folder_path = os.getcwd()+r'\\tfrecords'\n",
    "#folder_path = \"C:\\Users\\Jesper\\Desktop\\DataGeneration\\Data_Outputs\"\n",
    "\n",
    "#load filenames of folder: \n",
    "tfrecord_files = os.listdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outtfpath = os.getcwd()+r'\\processed_tfrecords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imgs: float\n",
    "#num_words: int\n",
    "#vf: float\n",
    "#edge index: int\n",
    "#adj int \n",
    "\n",
    "options = tf.compat.v1.io.TFRecordOptions(tf.compat.v1.io.TFRecordCompressionType.GZIP)\n",
    "\n",
    "for idx, record in enumerate(tfrecord_files):\n",
    "    tfrecord_path = os.path.join(folder_path,record)\n",
    "    dataset = TFRecordDataset(tfrecord_path, index_path, tfrecord_description)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batchsize)\n",
    "    for elem in loader:\n",
    "        dd = tfrecord_transforms(elem,batch_size=batchsize)\n",
    "        #\n",
    "        imgs = dd['imgs'].numpy()\n",
    "        \n",
    "        #\n",
    "        nw = dd['num_words'].numpy()\n",
    "        \n",
    "        #\n",
    "        vf = np.array(dd['vertex_features'])\n",
    "        \n",
    "        #\n",
    "        edge_num = []\n",
    "        for e in dd['edge_index']:\n",
    "            edge_num.append(e.shape[1])\n",
    "        \n",
    "        edge_num = np.array(edge_num,dtype=int)\n",
    "        \n",
    "        #\n",
    "        edge_indexes = []\n",
    "        maxed = np.max(edge_num)\n",
    "        for i,ed in enumerate(dd['edge_index']):\n",
    "            a = np.zeros(maxed)\n",
    "            b = np.zeros(maxed)\n",
    "\n",
    "            a[:edge_num[i]] = ed[0]\n",
    "            b[:edge_num[i]] = ed[1]\n",
    "\n",
    "            edge_indexes.append(np.array([a,b],dtype=int))\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        out = dict()\n",
    "        \n",
    "        out['imgs'] = tf.train.Feature(float_list=tf.train.FloatList(value=imgs.flatten()))\n",
    "        out['num_words'] = tf.train.Feature(int64_list=tf.train.Int64List(value=nw.flatten()))\n",
    "        out['vertex_features'] = tf.train.Feature(float_list=tf.train.FloatList(value=vf.flatten()))\n",
    "        out['num_edges'] =  tf.train.Feature(int64_list=tf.train.Int64List(value=edge_num.flatten()))\n",
    "        out['edge_indexes'] =  tf.train.Feature(int64_list=tf.train.Int64List(value=np.array(edge_indexes).flatten()))\n",
    "        \n",
    "        out['adjacency_matrix_cells'] = tf.train.Feature(int64_list=tf.train.Int64List(value=np.array(dd['adjacency_matrix_cells']).flatten()))\n",
    "        out['adjacency_matrix_cols'] = tf.train.Feature(int64_list=tf.train.Int64List(value=np.array(dd['adjacency_matrix_cols']).flatten()))\n",
    "        out['adjacency_matrix_rows'] = tf.train.Feature(int64_list=tf.train.Int64List(value=np.array(dd['adjacency_matrix_rows']).flatten()))\n",
    "        \n",
    "    with tf.io.TFRecordWriter(os.path.join(outtfpath,record+\".gz\"),options=options) as writer:\n",
    "        \n",
    "        all_features = tf.train.Features(feature=out)\n",
    "\n",
    "\n",
    "        seq_ex = tf.train.Example(features=all_features)\n",
    "        writer.write(seq_ex.SerializeToString()) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['adjacency_matrix_rows']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_indexes = []\n",
    "maxed = np.max(edge_num)\n",
    "for i,ed in enumerate(dd['edge_index']):\n",
    "    a = np.zeros(maxed)\n",
    "    b = np.zeros(maxed)\n",
    "    \n",
    "    a[:edge_num[i]] = ed[0]\n",
    "    b[:edge_num[i]] = ed[1]\n",
    "    \n",
    "    edge_indexes.append(np.array([a,b]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[:temp[0]] = ed[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = None\n",
    "tfrecord_description = {\"imgs\": \"float\", \n",
    "               \"num_words\": \"int\",\n",
    "               \"vertex_features\": \"float\",\n",
    "               \"adjacency_matrix_cells\":\"int\",\n",
    "               \"adjacency_matrix_cols\":\"int\",\n",
    "               \"adjacency_matrix_rows\":\"int\",\n",
    "               \"num_edges\":'int',\n",
    "               \"edge_indexes\" : 'int'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.getcwd()+r'\\processed_tfrecords'\n",
    "#folder_path = \"C:\\Users\\Jesper\\Desktop\\DataGeneration\\Data_Outputs\"\n",
    "\n",
    "#load filenames of folder: \n",
    "tfrecord_files = os.listdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['25GAK2KR3GYCHBO0BY58.tfrecord.gz']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfrecord_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "record= tfrecord_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.compat.v1.io.TFRecordOptions(tf.compat.v1.io.TFRecordCompressionType.GZIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_path = os.path.join(folder_path,record)\n",
    "dataset = TFRecordDataset(tfrecord_path, index_path, tfrecord_description)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to read the record.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-ef4ad6e5b854>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0melem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\jesper\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jesper\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jesper\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                     \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jesper\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tfrecord\\reader.py\u001b[0m in \u001b[0;36mtfrecord_loader\u001b[1;34m(data_path, index_path, description, shard)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[0mrecord_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfrecord_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshard\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrecord_iterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[0mexample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexample_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[0mexample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jesper\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tfrecord\\reader.py\u001b[0m in \u001b[0;36mtfrecord_iterator\u001b[1;34m(data_path, index_path, shard)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mindex_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mread_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jesper\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tfrecord\\reader.py\u001b[0m in \u001b[0;36mread_records\u001b[1;34m(start_offset, end_offset)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mdatum_bytes_view\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatum_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatum_bytes_view\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Failed to read the record.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrc_bytes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Failed to read the end token.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to read the record."
     ]
    }
   ],
   "source": [
    "elem = next(iter(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imgs: float\n",
    "num_words: int\n",
    "vf: float\n",
    "edge index: int\n",
    "adj int "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
