{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tfrecord\n",
    "import cv2\n",
    "from time import process_time\n",
    "import tensorflow as tf\n",
    "from tfrecord.torch.dataset import TFRecordDataset\n",
    "import os\n",
    "from feature_CNN import FeatureNet_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visibility_matrix(torch_df,num_words):\n",
    "    '''indentify neighbours to the right and down and generate visibility matrix / neighbourhood graph.\n",
    "        for each node, we indentify it's closest neighbour to the right and the closest neighbour below.\n",
    "    input: numpy array of shape (words, [x1, x2, y1, y2])\n",
    "    output: visibility matrix of shape (words, words)'''\n",
    "    \n",
    "    #remove last column (word_length)\n",
    "    npdf = torch_df.numpy()\n",
    "    \n",
    "    #Only create matrix of size matching number of words\n",
    "    matrix = np.zeros((num_words, num_words))\n",
    "\n",
    "    for i,row1 in enumerate(npdf):\n",
    "        if i == num_words:\n",
    "            break\n",
    "\n",
    "        #xmin = 0\n",
    "        #ymin = 1\n",
    "        #xmax = 2\n",
    "        #ymax = 3 \n",
    "\n",
    "        min_down = 10**6\n",
    "        min_right = 10**6\n",
    "        min_down_idx = None\n",
    "        min_right_idx = None\n",
    "\n",
    "        for j,row2 in enumerate(npdf):\n",
    "            if j == num_words:\n",
    "                break\n",
    "            if i != j:\n",
    "                #Right neighbour\n",
    "                if row1[1] <= row2[1] <= row1[3] or row1[1] <= row2[3] <= row1[3] or row2[1] <= row1[1] <= row2[3] or row2[1] <= row1[3] <= row2[3]:\n",
    "                    if  0 <= row2[0]-row1[2] <= min_right:\n",
    "                        min_right_idx, min_right = j, row2[0]-row1[2]\n",
    "\n",
    "                #Down neighbour\n",
    "                if row1[0] <= row2[0] <= row1[2] or row1[0] <= row2[2] <= row1[2] or row2[0] <= row1[0] <= row2[2] or row2[0] <= row1[2] <= row2[2]:\n",
    "                    if 0 <= row2[1]-row1[3] <= min_down:\n",
    "                        min_down_idx, min_down = j, row2[1]-row1[3]\n",
    "\n",
    "        if min_right_idx != None:\n",
    "            matrix[i,min_right_idx] = 1\n",
    "            matrix[min_right_idx, i] = 1    \n",
    "        if min_down_idx != None:\n",
    "            matrix[i,min_down_idx] = 1\n",
    "            matrix[min_down_idx, i] = 1\n",
    "            \n",
    "    source = []\n",
    "    target = []\n",
    "\n",
    "    for i, row in enumerate(matrix):\n",
    "        for j, edge in enumerate(row):\n",
    "            if edge == 1:\n",
    "                source.append(i)\n",
    "                target.append(j)\n",
    "\n",
    "    edge_index = torch.tensor([source, target], dtype=torch.long)\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "\n",
    "def tfrecord_transforms(elem,\n",
    "                   device,\n",
    "                   max_height = 768,\n",
    "                   max_width = 1366,\n",
    "                   num_of_max_vertices = 250,\n",
    "                   max_length_of_word = 30,\n",
    "                   batch_size = 8):\n",
    "    \"\"\"\n",
    "    Function used to transform the data loaded by the TFRecord dataloader.\n",
    "    Parameters are defind in TIES datageneration, defines the size and complexity of the generated tables. DO NOT CHANGE  \n",
    "    \"\"\"\n",
    "    reshape = 0\n",
    "    xnumwords = 0\n",
    "    feat_reshap = 0\n",
    "    visimat = 0\n",
    "    adjmats = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #Everything is flattened in tfrecord, so needs to be reshaped. \n",
    "\n",
    "        #Images are in range [0,255], need to be in [0,1]\n",
    "        #If image max is over 1 , then normalize: \n",
    "        data_dict =  {}\n",
    "\n",
    "        \n",
    "        #Torch dimensions: B x C x H x W\n",
    "        #inputting grayscale, so only 1 dimension\n",
    "        t = process_time()\n",
    "        if torch.max(elem['image']) > 1:\n",
    "            data_dict['imgs'] = (elem['image']/255).reshape(batch_size,1,max_height,max_width).to(device)\n",
    "        else:\n",
    "            data_dict['imgs'] = elem['image'].reshape(batch_size,1,max_height,max_width).to(device)\n",
    "        reshape+=process_time()-t\n",
    "\n",
    "        #Extract number of words for each image:\n",
    "        t = process_time()\n",
    "        num_words = elem['global_features'][:,2]\n",
    "        data_dict['num_words'] = num_words.to(device)\n",
    "        xnumwords += process_time()-t\n",
    "        \n",
    "        t = process_time()\n",
    "        v = elem['vertex_features'].reshape(batch_size,num_of_max_vertices,5).float()\n",
    "        feat_reshap += process_time()-t\n",
    "        #normalizaing words coordinates to be invariant to image size \n",
    "        v[:,:,0] = v[:,:,0]/max_width\n",
    "        v[:,:,1] = v[:,:,1]/max_height\n",
    "        v[:,:,2] = v[:,:,2]/max_width\n",
    "        v[:,:,3] = v[:,:,3]/max_height\n",
    "\n",
    "        #data_dict['vertex_features'] = v\n",
    "\n",
    "        vertex_feats = []\n",
    "        for idx,vf in enumerate(v):\n",
    "            tmp = vf[0:num_words[idx]].to(device)\n",
    "            #tmp.requires_grad=True\n",
    "            vertex_feats.append(tmp)\n",
    "\n",
    "        data_dict['vertex_features'] = vertex_feats  \n",
    "                \n",
    "        #Calculate visibility matrix for each batch element\n",
    "        t = process_time()\n",
    "        edge_index = []\n",
    "        for idx,vex in enumerate(v):\n",
    "            edge_index.append(visibility_matrix(vex,num_words[idx]).to(device))\n",
    "        visimat += process_time()-t\n",
    "         \n",
    "        data_dict['edge_index'] = edge_index\n",
    "\n",
    "        \n",
    "        adj_cells = []\n",
    "        adj_cols = []\n",
    "        adj_rows = []\n",
    "        for idx,nw in enumerate(num_words):\n",
    "            adj_cells.append(elem['adjacency_matrix_cells'][idx].reshape(num_of_max_vertices,num_of_max_vertices)[:nw][:nw].to(device))\n",
    "            adj_cols.append(elem['adjacency_matrix_cols'][idx].reshape(num_of_max_vertices,num_of_max_vertices)[:nw][:nw].to(device))\n",
    "            adj_rows.append(elem['adjacency_matrix_rows'][idx].reshape(num_of_max_vertices,num_of_max_vertices)[:nw][:nw].to(device))\n",
    "\n",
    "        data_dict['adjacency_matrix_cells'] = adj_cells\n",
    "        data_dict['adjacency_matrix_cols'] = adj_cols\n",
    "        data_dict['adjacency_matrix_rows'] = adj_rows\n",
    "        \n",
    "\n",
    "        \n",
    "        print(f'#####TRANSFORMS: reshape: {reshape}, extract number of words: {xnumwords}, feat_reshape: {feat_reshap}, visibility matrix: {visimat}, adjacency matrix: {adjmats}')\n",
    "\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables for tfrecord loader\n",
    "batchsize = 8\n",
    "index_path = None\n",
    "tfrecord_description = {\"image\": \"float\", \n",
    "               \"global_features\": \"int\",\n",
    "               \"vertex_features\": \"int\",\n",
    "               \"adjacency_matrix_cells\":\"int\",\n",
    "               \"adjacency_matrix_cols\":\"int\",\n",
    "               \"adjacency_matrix_rows\":\"int\",\n",
    "               \"vertex_text\":'int'}\n",
    "\n",
    "#Load list of tfRecords from folder: \n",
    "folder_path = os.getcwd()+r'\\tfrecords'\n",
    "#folder_path = \"C:\\Users\\Jesper\\Desktop\\DataGeneration\\Data_Outputs\"\n",
    "\n",
    "#load filenames of folder: \n",
    "tfrecord_files = os.listdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Jesper\\\\Desktop\\\\TableRecognition\\\\Table_Detection_and_Recognition\\\\Table_Recognition\\\\tfrecords'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = tfrecord_files[0]\n",
    "tfrecord_path = os.path.join(folder_path,record)\n",
    "dataset = TFRecordDataset(tfrecord_path, index_path, tfrecord_description)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureNet_v1(\n",
       "  (conv_1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv_2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool_1): MaxPool2d(kernel_size=5, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_3): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool_2): AvgPool2d(kernel_size=5, stride=3, padding=0)\n",
       "  (dropout): Dropout2d(p=0.5, inplace=False)\n",
       "  (l_1): Linear(in_features=888832, out_features=128, bias=True)\n",
       "  (l_out): Linear(in_features=128, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "#load Feature CNN model\n",
    "featurenet_path = os.getcwd()+r\"\\models\\FeatureNet_v1.pt\"\n",
    "featurenet = FeatureNet_v1()\n",
    "featurenet.load_state_dict(torch.load(featurenet_path,map_location=torch.device('cpu')))\n",
    "featurenet.eval()\n",
    "featurenet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1049088])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####TRANSFORMS: reshape: 0.78125, extract number of words: 0.0, feat_reshape: 0.0, visibility matrix: 5.21875, adjacency matrix: 0\n"
     ]
    }
   ],
   "source": [
    "data_dict = tfrecord_transforms(batch,device=device,batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = featurenet.feature_forward(data_dict['imgs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 124, 224])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_map[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "pxl_l, pxl_h = 1/feature_map.shape[2], 1/feature_map.shape[3]\n",
    "max_l, max_h = 0,0\n",
    "max_x, max_y = feature_map.shape[2], feature_map.shape[3]\n",
    "\n",
    "for batch in range(batchsize):\n",
    "    x1, y1, x2, y2 = data_dict['vertex_features'][batch][:,0].cpu().numpy(), data_dict['vertex_features'][batch][:,1].cpu().numpy(), data_dict['vertex_features'][batch][:,2].cpu().numpy(), data_dict['vertex_features'][batch][:,3].cpu().numpy()\n",
    "    l, h = x2-x1,y2-y1\n",
    "    if max_l < np.max(l):\n",
    "        max_l = np.max(l)\n",
    "    if max_h < np.max(h):\n",
    "        max_h = np.max(h)\n",
    "        \n",
    "max_l, max_h = np.ceil(max_l/pxl_l),np.ceil(max_h/pxl_h)\n",
    "\n",
    "gathered_feats = []\n",
    "for batch in range(batchsize):\n",
    "    all_feats_img = []\n",
    "    for idx, word in enumerate(data_dict['vertex_features'][batch]):\n",
    "        x1, y1, x2, y2, _ = word.cpu().numpy()\n",
    "        l, h = x2-x1,y2-y1\n",
    "        c = np.floor(((x1+l)/pxl_l,(y1+h)/pxl_h))\n",
    "\n",
    "        x_slice = int(c[0]-np.floor(max_l/2)),int(c[0]+np.ceil(max_l/2))\n",
    "        if min(x_slice) < 0:\n",
    "            x_slice = (x_slice[0]+abs(min(x_slice)),x_slice[1]+abs(min(x_slice)))\n",
    "        if max(x_slice) > max_x:\n",
    "            x_slice = (x_slice[0]-(max(x_slice)-max_x),x_slice[1]-(max(x_slice)-max_x))\n",
    "            \n",
    "        y_slice = int(c[1]-np.floor(max_h/2)),int(c[1]+np.ceil(max_h/2))\n",
    "        if min(y_slice) < 0:\n",
    "            y_slice = (y_slice[0]+abs(min(y_slice)),y_slice[1]+abs(min(y_slice)))\n",
    "        if max(y_slice) > max_y:\n",
    "            y_slice = (y_slice[0]-(max(y_slice)-max_y),y_slice[1]-(max(y_slice)-max_y))\n",
    "        \n",
    "        all_feats_w = word\n",
    "        for layer in range(feature_map.shape[1]):\n",
    "            featmapfeats = feature_map[batch][layer][x_slice[0]:x_slice[1],y_slice[0]:y_slice[1]]\n",
    "            all_feats_w = torch.cat((all_feats_w,torch.flatten(featmapfeats)))\n",
    "        all_feats_img.append(all_feats_w)\n",
    "\n",
    "    gathered_feats.append(torch.stack(all_feats_img,dim=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
